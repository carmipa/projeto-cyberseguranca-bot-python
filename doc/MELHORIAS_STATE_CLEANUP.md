# üßπ Melhorias no Sistema de Limpeza do state.json

**Data:** 13 de Fevereiro de 2026  
**Problema:** `state.json` pode crescer excessivamente causando problemas de performance

---

## üéØ Problema Identificado

O arquivo `state.json` armazena:
- **`dedup`**: Hist√≥rico de links processados por feed (pode crescer muito)
- **`http_cache`**: Cache HTTP com ETags e Last-Modified (pode crescer)
- **`html_hashes`**: Hashes de sites monitorados (crescimento moderado)

**Problema:** Quando o arquivo enche (> 10MB), causa:
- Lentid√£o ao carregar/salvar
- Alto uso de mem√≥ria
- Poss√≠veis timeouts
- Corrup√ß√£o em casos extremos

---

## ‚úÖ Solu√ß√£o Implementada

### 1. M√≥dulo Dedicado de Limpeza (`utils/state_cleanup.py`)

Criado m√≥dulo especializado com fun√ß√µes:
- `check_and_cleanup_state()` - Verifica e limpa automaticamente
- `cleanup_state()` - Executa limpeza com estat√≠sticas
- `should_cleanup_by_time()` - Verifica limpeza por tempo (7 dias)
- `should_cleanup_by_size()` - Verifica limpeza por tamanho

### 2. Limpeza Inteligente Multi-Crit√©rio

**Crit√©rios de Limpeza:**

#### A) Por Tempo (7 dias)
- Limpeza autom√°tica a cada 7 dias
- Previne crescimento gradual

#### B) Por Tamanho Cr√≠tico (> 10 MB)
- Limpeza imediata se arquivo > 10 MB
- Previne problemas de performance

#### C) Por Tamanho de Aviso (> 5 MB)
- Limpeza parcial se arquivo > 5 MB
- Mant√©m dados recentes, remove antigos

### 3. Limpeza Seletiva por Se√ß√£o

**Limites por Se√ß√£o:**
- **`dedup`**: M√°ximo 2000 itens totais
  - Se feed individual > 500 itens, mant√©m √∫ltimos 500
- **`http_cache`**: M√°ximo 1000 itens
- **`html_hashes`**: M√°ximo 100 itens

**Estrat√©gia:**
- Limpeza completa quando necess√°rio (tempo ou tamanho cr√≠tico)
- Limpeza parcial quando arquivo grande mas n√£o cr√≠tico
- Preserva `html_hashes` quando poss√≠vel (importante para monitoramento)

---

## üìä Estat√≠sticas e Logs

### Logs Detalhados

```
üßπ [Auto-Cleanup] Executando limpeza completa (Ciclo de 7 dias)
üìä Antes: dedup=5000, cache=2000, hashes=50, tamanho=8.50 MB
üßπ Limpando dedup: 5000 itens -> 0
üßπ Limpando http_cache: 2000 itens -> 0
‚úÖ Mantendo html_hashes (50 entradas)
‚úÖ Limpeza conclu√≠da. Novo tamanho: 0.15 MB (redu√ß√£o de 8.35 MB)
```

### M√©tricas Coletadas

- Tamanho do arquivo antes/depois
- Contagem de itens por se√ß√£o antes/depois
- Motivo da limpeza (tempo/tamanho)
- Redu√ß√£o de tamanho alcan√ßada

---

## üîß Configura√ß√£o

### Limites Configur√°veis (`utils/state_cleanup.py`)

```python
# Limites de tamanho de arquivo
MAX_STATE_SIZE = 10 * 1024 * 1024  # 10 MB - cr√≠tico
WARN_STATE_SIZE = 5 * 1024 * 1024  # 5 MB - aviso

# Intervalo de limpeza por tempo
CLEANUP_INTERVAL = 604800  # 7 dias em segundos

# Limites de itens por se√ß√£o
MAX_DEDUP_ITEMS = 2000
MAX_CACHE_ITEMS = 1000
MAX_HASHES_ITEMS = 100
```

### Ajustar Limites

Para ambientes com muitos feeds ou alta frequ√™ncia:

```python
# Aumentar limites
MAX_DEDUP_ITEMS = 5000
MAX_CACHE_ITEMS = 2000

# Reduzir intervalo de limpeza (mais frequente)
CLEANUP_INTERVAL = 259200  # 3 dias
```

---

## üöÄ Uso

### Autom√°tico (Recomendado)

A limpeza acontece automaticamente no scanner:

```python
# Em core/scanner.py
from utils.state_cleanup import check_and_cleanup_state
state = check_and_cleanup_state(force=False)
```

### Manual (Para Debug/Manuten√ß√£o)

```python
from utils.state_cleanup import check_and_cleanup_state, cleanup_state
from utils.storage import p, load_json_safe, save_json_safe

# Verifica e limpa se necess√°rio
state = check_and_cleanup_state(force=False)

# For√ßa limpeza imediata
state = check_and_cleanup_state(force=True)

# Limpeza manual com controle total
state = load_json_safe(p("state.json"), {})
state, stats = cleanup_state(state, reason="Manuten√ß√£o manual")
save_json_safe(p("state.json"), state, atomic=True)
```

---

## üìà Benef√≠cios

### Performance
- ‚úÖ Arquivo sempre em tamanho gerenci√°vel
- ‚úÖ Carregamento/salvamento r√°pido
- ‚úÖ Menor uso de mem√≥ria

### Confiabilidade
- ‚úÖ Previne corrup√ß√£o por arquivo muito grande
- ‚úÖ Evita timeouts em opera√ß√µes de I/O
- ‚úÖ Sistema mais est√°vel

### Manuten√ß√£o
- ‚úÖ Limpeza autom√°tica sem interven√ß√£o
- ‚úÖ Logs detalhados para auditoria
- ‚úÖ Configur√°vel para diferentes ambientes

---

## üîç Monitoramento

### Verificar Tamanho Atual

```python
from utils.state_cleanup import get_state_size
from utils.storage import p

size = get_state_size(p("state.json"))
print(f"Tamanho atual: {size / 1024 / 1024:.2f} MB")
```

### Verificar √öltima Limpeza

```python
from utils.storage import p, load_json_safe
from datetime import datetime

state = load_json_safe(p("state.json"), {})
last_clean = state.get("last_cleanup", 0)
if last_clean:
    last_clean_dt = datetime.fromtimestamp(last_clean)
    print(f"√öltima limpeza: {last_clean_dt}")
else:
    print("Nunca foi limpo")
```

---

## ‚ö†Ô∏è Notas Importantes

1. **Deduplica√ß√£o Tempor√°ria**
   - Ap√≥s limpeza completa, pode haver posts duplicados temporariamente
   - O sistema se recupera automaticamente na pr√≥xima varredura
   - `history.json` ainda previne duplicatas cr√≠ticas

2. **Cache HTTP**
   - Limpeza do cache HTTP pode causar mais requisi√ß√µes
   - Impacto m√≠nimo, pois feeds s√£o verificados periodicamente
   - Benef√≠cio de performance supera custo de requisi√ß√µes extras

3. **HTML Hashes**
   - Preservados quando poss√≠vel
   - Limpados apenas se muito grandes (> 100)
   - Pode causar detec√ß√£o de "mudan√ßa" em sites monitorados ap√≥s limpeza

---

## ‚úÖ Conclus√£o

O sistema de limpeza agora √©:
- ‚úÖ **Autom√°tico** - N√£o requer interven√ß√£o manual
- ‚úÖ **Inteligente** - Limpa baseado em m√∫ltiplos crit√©rios
- ‚úÖ **Seguro** - Preserva dados importantes quando poss√≠vel
- ‚úÖ **Audit√°vel** - Logs detalhados de todas as opera√ß√µes
- ‚úÖ **Configur√°vel** - Ajust√°vel para diferentes ambientes

**Problema resolvido:** `state.json` n√£o vai mais causar problemas de performance por crescimento excessivo.

---

*Melhorias implementadas em 13 de Fevereiro de 2026*
